<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory">
  <meta name="keywords" content="Robot memory, vision-language models, task planning, Learning from Experience">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- Prism.js CSS -->
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.css" rel="stylesheet" />

  <!-- Prism.js JS -->
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

  <script src="./static/js/prompts.js"></script>
  <script src="./static/js/examples.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/github-mark.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- 
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 is-size-2 publication-title">Experience is the Best Teacher: Grounding VLMs for
              Robotics through Self-Generated Memory</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kaixian-qu-66a86215a/?originalSubdomain=ch">Kaixian
                  Qu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.jie-tan.net/">Jie Tan</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/tingnanzhang/">Tingnan Zhang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://fxia22.github.io/">Fei Xia</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://n.ethz.ch/~cesarc/">Cesar Cadena</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://rsl.ethz.ch/the-lab/people/person-detail.MTIxOTEx.TGlzdC8yNDQxLC0xNDI1MTk1NzM1.html">Marco
                  Hutter</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Robotic Systems Lab, ETH Zurich</span>
              <span class="author-block"><sup>2</sup>Google DeepMind</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.19697" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.19697" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://github.com/google/nerfies" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://github.com/google/nerfies/releases/tag/0.1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <!-- Animation. -->
      <section class="hero teaser">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/find_8_objects_10x_fastforward.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              The robot finds eight different objects in succession: a toy elephant, a screwdriver, a microwave oven, a
              coffee machine, a sofa, a plant, a chair, and a TV. We send the next target immediately when the robot
              locates the current object of interest and the robot can reuse the probability map and ESDF for an
              efficient search.
            </h2>
          </div>
        </div>
      </section>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toy_elephant_animation_5x.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/microwave_oven_animation_10x.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/plant_3x.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ping_pong_paddles_3x.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision-language models (VLMs) have been widely adopted in robotics to enable autonomous planning. However,
              grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge.
              This paper presents ExpTeach, a framework that grounds VLMs to physical robots by building a
              self-generated memory of real-world experiences. In ExpTeach, the VLM autonomously plans actions, verifies
              outcomes, reflects on failures, and adapts robot behaviors in a closed loop. The self-generated
              experiences during this process are then summarized into a long-term memory, enabling retrieval of learned
              knowledge to guide future tasks via retrieval-augmented generation (RAG). Additionally, ExpTeach enhances
              the spatial understanding of VLMs with an on-demand image annotation module. In experiments, we show that
              reflection improves success rates from 36% to 84% on four challenging robotic tasks and observe the
              emergence of intelligent object interactions, including creative tool use. Across extensive tests on 12
              real-world scenarios (including eight unseen ones), we find that grounding with long-term memory boosts
              single-trial success rates from 22% to 80%, demonstrating the effectiveness and generalizability of
              ExpTeach.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video id="dollyzoom" controls playsinline height="100%">
              <source src="./static/videos/ippon_icra_explanantion.mp4" type="video/mp4">
            </video>
            <!-- <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <img src="./static/images/pipeline_figure.jpeg" class="interpolation-image"
            alt="ExpTeach pipleline figure." />
          <p>
            At the start of each task, the system takes the user instruction \( \mathbf{I} \)
            and egocentric observation \( \mathbf{o}_{0} \), which the VLM summarizes into a scenario.
            RAG retrieves relevant experiences from long-term memory \( \mathbf{M} \) and, together with the instruction
            and observation,
            feeds them into the VLM task planner \( \mathcal{T} \).
            After execution, success is checked by the VLM. If the task is not completed,
            the action \( \mathbf{a} \) and its feedback \( \mathbf{r} \) are accumulated into short-term memory \(
            \mathbf{m} \) and fed back into planning.
            Once the task is completed, the short-term memory \( \mathbf{m} \) is summarized and stored in long-term
            memory \( \mathbf{M} \)
            for future use.
          </p>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="columns is-centered">

        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Habitat ObjectNav Evaluation</h2>
            <table>
              <caption>Evaluation results on the Habitat ObjectNav 2023 Challenge (standard phase)</caption>
              <thead>
                <tr>
                  <th>Method</th>
                  <th>SPL &#x2191;</th>
                  <th>Soft SPL &#x2191;</th>
                  <th>Success &#x2191;</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><a href="https://eval.ai/web/challenges/challenge-page/1992/leaderboard/4704">Host team</a></td>
                  <td>0.05</td>
                  <td>0.27</td>
                  <td>0.12</td>
                </tr>
                <tr style="background-color: #f2f2f2;">
                  <td><a href="https://ieeexplore.ieee.org/abstract/document/10477345">Auxiliary RL</a></td>
                  <td>0.10</td>
                  <td>0.31</td>
                  <td>0.18</td>
                </tr>
                <tr>
                  <td><a href="https://eval.ai/web/challenges/challenge-page/1992/leaderboard/4704">ICanFly</a></td>
                  <td>0.26</td>
                  <td>0.37</td>
                  <td>0.43</td>
                </tr>
                <tr style="background-color: #f2f2f2;">
                  <td><a href="https://eval.ai/web/challenges/challenge-page/1992/leaderboard/4704">SkillFusion</a></td>
                  <td>0.28</td>
                  <td>0.34</td>
                  <td>0.53</td>
                </tr>
                <tr>
                  <td><a href="https://ieeexplore.ieee.org/abstract/document/10477345">SkillTron</a></td>
                  <td>0.28</td>
                  <td>0.36</td>
                  <td><strong>0.59</strong></td>
                </tr>
                <tr style="background-color: #f2f2f2;">
                  <td>IPPON (ours)</td>
                  <td><strong>0.34</strong></td>
                  <td><strong>0.46</strong></td>
                  <td>0.54</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

      </div>
    </div>
  </section>

  <!-- Prompts -->
  <section class="hero is-light is-small" id="llm">
    <section class="section">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Prompts</h2>

          <div class="columns">
            <div class="column has-text-centered">
              <div class="select is-large">
                <select id="llm-prompt-menu" onchange="setPlaceholder()">
                  <option value="template" selected="selected">Please select one VLM module</option>
                  <option value="scene-describer">VLM Scene Describer</option>
                  <option value="action-planner">VLM Action Planner</option>
                  <option value="grasping-selector">VLM Grasping Selector</option>
                  <option value="placement-selector">VLM Placement Selector</option>
                  <option value="pushing-spot-selector">VLM Pushing Spot Selector</option>
                  <option value="success-detector">VLM Success Detector</option>
                  <option value="experience-summarizer">VLM Experience Summarizer</option>
                </select>
              </div>
            </div>
          </div>

          <div class="columns is-centered is-four-fifths">
            <div class="column">
              <div class="content">
                <h2 class="title is-4">Description</h2>
                <textarea id="DescriptionPlaceholder" placeholder="
Please select one VLM module to see the prompt.
" readonly></textarea>
              </div>
            </div>

            <div class="column">
              <div class="content">
                <h2 class="title is-4">Prompt</h2>
                <textarea id="PromptPlaceholder" placeholder="
" readonly></textarea>
              </div>
            </div>

          </div>
        </div>
      </div>
    </section>
  </section>

  <section style="height: 40px; background-color: white;"></section>

  <!-- Response examples -->
  <section class="hero is-light is-small" id="example">
    <section class="section">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples</h2>

          <div class="columns">
            <div class="column has-text-centered">
              <div class="select is-large">
                <select id="example-menu" onchange="showSelectedContainer()">
                  <option value="default" selected="selected">Please select one effect.</option>
                  <option value="self-reflection">Self-Reflection</option>
                  <option value="experience-summarization">Experience Summarization</option>
                  <option value="experience-retrieval">Experience Retrieval</option>
                  <option value="grounding-with-memory">Grounding with Memory</option>
                  <option value="image-annotation">Image Annotation</option>
                </select>
              </div>
            </div>
          </div>

          <div id="default-container">
            <p>Please select a module from the dropdown to view its details.</p>
          </div>

          <div class="module-container" id="self-reflection-container" style="display:none;">
            <h2 class="title is-4">Instruction: Put the apple on the plate.</h2>

            <table class="table is-fullwidth is-striped" id="tab:stm_building2">
              <thead>
                <tr>
                  <th style="width: 30%;">Scene</th>
                  <th style="width: 70%;">VLM Response</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><img src="./static/images/stm_apple_can/apple_can_1.png"
                      alt="Apple next to salt container, plate on left"></td>
                  <td class="code">
Response from VLM Task Planner: {
    "name": "pick_object",
    "arguments": {
        "scene_description": "The apple is on the right side of the table, next to a salt container. The plate is on the left side of the table, with ample space around it.",
        "reasoning": "The apple needs to be picked up first to be placed on the plate. The apple is isolated enough to be grasped without interference from the salt container.",
        "target_object": "apple",
        "grasp_part": "",
        "specific_grasp_required": false,
        "action_description": "Pick up the apple from the table."
    }
}
                  </td>
                </tr>
                <tr>
                  <td><img src="./static/images/stm_apple_can/apple_can_2.png" alt="Robot failed to pick up apple"></td>
                  <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The apple is still on the table, to the right of the plate and next to the salt container. The robot's gripper is empty and positioned at the edge of the table, indicating that the action to pick up the apple has not been completed.",
        "action_status": "failed",
        "failure_cause": "The apple has not been picked up; the gripper is empty and the apple remains on the table",
        "next_step_suggestions": "Reposition the gripper to approach the apple from a different angle, or move the salt container to create more space if needed, then attempt to grasp the apple again.",
        "is_task_completed": false
    }
}
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td class="code">
Response from VLM Task Planner: {
    "name": "push_object",
    "arguments": {
        "scene_description": "The apple is on the table, positioned between a plate on the left and a salt container on the right. The salt container is close to the apple, potentially obstructing the gripper's approach.",
        "reasoning": "The previous attempt to pick up the apple failed, possibly due to the proximity of the salt container. Pushing the salt container to the right will create more space for the gripper to approach the apple from a better angle.",
        "object_to_push": "salt container",
        "push_direction": "right",
        "action_description": "Push the salt container to the right to create more space around the apple."
    }
}
                  </td>
                </tr>
                <tr>
                  <td><img src="./static/images/stm_apple_can/apple_can_3.png"
                      alt="Salt container pushed right, space around apple"></td>
                  <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action was to push the salt container to the right to create more space around the apple, making it easier for the robot to grasp the apple and place it on the plate. In the current image, the salt container is already positioned to the right of the apple, and there is a clear gap between the apple and the salt container. The apple is not obstructed by the salt container, and there is sufficient space for the robot's gripper to approach the apple from multiple angles. The action appears to have been completed successfully, as the intended result (creating space around the apple) has been achieved.",
        "action_status": "successful",
        "failure_cause": "None",
        "next_step_suggestions": "Proceed to grasp the apple and move it onto the plate.",
        "is_task_completed": false
    }
}
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td class="code">
Response from VLM Task Planner: {
    "name": "pick_object",
    "arguments": {
        "scene_description": "The apple is on the table, with a plate to its left and a salt container further to the right. The apple is unobstructed and can be directly accessed by the robot's gripper.",
        "reasoning": "The apple is now unobstructed, and the robot can directly attempt to grasp it. The previous failure suggests approaching from a different angle, but since the salt container has been moved, a direct approach should be feasible.",
        "target_object": "apple",
        "grasp_part": "",
        "specific_grasp_required": false,
        "action_description": "Approach and grasp the apple from above, ensuring a firm grip."
    }
}
                  </td>
                </tr>
                <tr>
                  <td><img src="./static/images/stm_apple_can/apple_can_4.png" alt="Robot holding apple">
                  </td>
                  <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The robot's gripper is currently holding the apple securely from above, as instructed. The apple is unobstructed and has been successfully grasped.",
        "action_status": "successful",
        "failure_cause": "None",
        "next_step_suggestions": "Move the gripper holding the apple above the plate and release the apple onto the plate.",
        "is_task_completed": false
    }
}
                  </td>
                </tr>
                <tr>
                  <td></td>
                  <td class="code">
Response from VLM Task Planner: {
    "name": "place_object",
    "arguments": {
        "scene_description": "The robot is holding an apple with its gripper. The plate is on the left side of the table, and there is enough space around it for placing the apple. The salt container is on the right side, away from the plate.",
        "reasoning": "The robot has already grasped the apple successfully. The next logical step is to move the apple over the plate and release it. The plate is clearly visible and accessible, with no obstructions in the way.",
        "target_object": "apple",
        "placement_location": "plate",
        "precise_placement_spot_required": false,
        "action_description": "Move the gripper holding the apple above the plate and release the apple onto the plate."
    }
}
                  </td>
                </tr>
                <tr>
                  <td><img src="./static/images/stm_apple_can/apple_can_5.png" alt="Apple placed on plate"></td>
                  <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The apple is already placed on the plate, and the robot's gripper is empty, indicating that the apple has been released.",
        "action_status": "successful",
        "failure_cause": "No failure; the apple is on the plate as instructed.",
        "next_step_suggestions": "No further action needed; task is complete.",
        "is_task_completed": true
    }
}
                  </td>
                </tr>
              </tbody>
            </table>
          </div>

        </div>
      </div>
    </section>
  </section>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
          </div>
    </section> -->


  <!-- Concurrent Work. -->
  <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an
            idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a
              href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a
              href="https://video-nerf.github.io/">Video-NeRF</a>, <a
              href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a
              href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a
              href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a
              href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
  <!--/ Concurrent Work. -->

  </div>

  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io"><span
                  class="dnerf">Nerfies</span></a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>