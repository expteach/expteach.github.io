<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory">
  <meta name="keywords" content="Robot memory, vision-language models, task planning, Learning from Experience">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <!-- Prism.js CSS -->
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.css" rel="stylesheet" />

  <!-- Prism.js JS -->
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

  <script src="./static/js/prompts.js"></script>
  <script src="./static/js/examples.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/github-mark.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- 
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 is-size-2 publication-title">Experience is the Best Teacher: Grounding VLMs for
              Robotics through Self-Generated Memory</h1>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kaixian-qu-66a86215a/?originalSubdomain=ch">Kaixian
                  Qu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.jie-tan.net/">Jie Tan</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/tingnanzhang/">Tingnan Zhang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://fxia22.github.io/">Fei Xia</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://n.ethz.ch/~cesarc/">Cesar Cadena</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://rsl.ethz.ch/the-lab/people/person-detail.MTIxOTEx.TGlzdC8yNDQxLC0xNDI1MTk1NzM1.html">Marco
                  Hutter</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Robotic Systems Lab, ETH Zurich</span>
              <span class="author-block"><sup>2</sup>Google DeepMind</span>
            </div> -->

            <!-- 
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2410.19697" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.19697" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://github.com/google/nerfies" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                <!-- <span class="link-block">
                  <a href="https://github.com/google/nerfies/releases/tag/0.1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>
            </div>
             -->
          </div>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <!-- Animation. -->
      <section class="hero teaser">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <video id="teaser" autoplay muted loop playsinline height="100%">
              <source src="./static/videos/find_8_objects_10x_fastforward.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
              The robot finds eight different objects in succession: a toy elephant, a screwdriver, a microwave oven, a
              coffee machine, a sofa, a plant, a chair, and a TV. We send the next target immediately when the robot
              locates the current object of interest and the robot can reuse the probability map and ESDF for an
              efficient search.
            </h2>
          </div>
        </div>
      </section>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toy_elephant_animation_5x.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/microwave_oven_animation_10x.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/plant_3x.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ping_pong_paddles_3x.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision-language models (VLMs) have been widely adopted in robotics to enable autonomous planning. However,
              grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge.
              This paper presents ExpTeach, a framework that grounds VLMs to physical robots by building a
              self-generated memory of real-world experiences. In ExpTeach, the VLM autonomously plans actions, verifies
              outcomes, reflects on failures, and adapts robot behaviors in a closed loop. The self-generated
              experiences during this process are then summarized into a long-term memory, enabling retrieval of learned
              knowledge to guide future tasks via retrieval-augmented generation (RAG). Additionally, ExpTeach enhances
              the spatial understanding of VLMs with an on-demand image annotation module. In experiments, we show that
              reflection improves success rates from 36% to 84% on four challenging robotic tasks and observe the
              emergence of intelligent object interactions, including creative tool use. Across extensive tests on 12
              real-world scenarios (including eight unseen ones), we find that grounding with long-term memory boosts
              single-trial success rates from 22% to 80%, demonstrating the effectiveness and generalizability of
              ExpTeach.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video id="dollyzoom" controls playsinline height="100%">
              <source src="./static/videos/ippon_icra_explanantion.mp4" type="video/mp4">
            </video>
            <!-- <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <img src="./static/images/pipeline_figure.jpeg" class="interpolation-image"
            alt="ExpTeach pipleline figure." />
          <p>
            At the start of each task, the system takes the user instruction \( \mathbf{I} \)
            and egocentric observation \( \mathbf{o}_{0} \), which the VLM summarizes into a scenario.
            RAG retrieves relevant experiences from long-term memory \( \mathbf{M} \) and, together with the instruction
            and observation,
            feeds them into the VLM task planner \( \mathcal{T} \).
            After execution, success is checked by the VLM. If the task is not completed,
            the action \( \mathbf{a} \) and its feedback \( \mathbf{r} \) are accumulated into short-term memory \(
            \mathbf{m} \) and fed back into planning.
            Once the task is completed, the short-term memory \( \mathbf{m} \) is summarized and stored in long-term
            memory \( \mathbf{M} \)
            for future use.
          </p>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <div class="columns is-centered">

        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Habitat ObjectNav Evaluation</h2>
            <table>
              <caption>Evaluation results on the Habitat ObjectNav 2023 Challenge (standard phase)</caption>
              <thead>
                <tr>
                  <th>Method</th>
                  <th>SPL &#x2191;</th>
                  <th>Soft SPL &#x2191;</th>
                  <th>Success &#x2191;</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><a href="https://eval.ai/web/challenges/challenge-page/1992/leaderboard/4704">Host team</a></td>
                  <td>0.05</td>
                  <td>0.27</td>
                  <td>0.12</td>
                </tr>
                <tr style="background-color: #f2f2f2;">
                  <td><a href="https://ieeexplore.ieee.org/abstract/document/10477345">Auxiliary RL</a></td>
                  <td>0.10</td>
                  <td>0.31</td>
                  <td>0.18</td>
                </tr>
                <tr>
                  <td><a href="https://eval.ai/web/challenges/challenge-page/1992/leaderboard/4704">ICanFly</a></td>
                  <td>0.26</td>
                  <td>0.37</td>
                  <td>0.43</td>
                </tr>
                <tr style="background-color: #f2f2f2;">
                  <td><a href="https://eval.ai/web/challenges/challenge-page/1992/leaderboard/4704">SkillFusion</a></td>
                  <td>0.28</td>
                  <td>0.34</td>
                  <td>0.53</td>
                </tr>
                <tr>
                  <td><a href="https://ieeexplore.ieee.org/abstract/document/10477345">SkillTron</a></td>
                  <td>0.28</td>
                  <td>0.36</td>
                  <td><strong>0.59</strong></td>
                </tr>
                <tr style="background-color: #f2f2f2;">
                  <td>IPPON (ours)</td>
                  <td><strong>0.34</strong></td>
                  <td><strong>0.46</strong></td>
                  <td>0.54</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

      </div>
    </div>
  </section>

  <!-- Prompts -->
  <section class="hero is-light is-small" id="llm">
    <section class="section">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Prompts</h2>

          <div class="columns">
            <div class="column has-text-centered">
              <div class="select is-large">
                <select id="llm-prompt-menu" onchange="setPlaceholder()">
                  <option value="template" selected="selected">Please select one VLM module</option>
                  <option value="scene-describer">VLM Scene Describer</option>
                  <option value="action-planner">VLM Action Planner</option>
                  <option value="grasping-selector">VLM Grasping Selector</option>
                  <option value="placement-selector">VLM Placement Selector</option>
                  <option value="pushing-spot-selector">VLM Pushing Spot Selector</option>
                  <option value="success-detector">VLM Success Detector</option>
                  <option value="experience-summarizer">VLM Experience Summarizer</option>
                </select>
              </div>
            </div>
          </div>

          <div class="columns is-centered is-four-fifths">
            <div class="column">
              <div class="content">
                <h2 class="title is-4">Description</h2>
                <textarea id="DescriptionPlaceholder" placeholder="
Please select one VLM module to see the prompt.
" readonly></textarea>
              </div>
            </div>

            <div class="column">
              <div class="content">
                <h2 class="title is-4">Prompt</h2>
                <textarea id="PromptPlaceholder" placeholder="
" readonly></textarea>
              </div>
            </div>

          </div>
        </div>
      </div>
    </section>
  </section>

  <section style="height: 40px; background-color: white;"></section>

  <!-- Response examples -->
  <section class="hero is-light is-small" id="example">
    <section class="section">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Examples</h2>

          <div class="columns">
            <div class="column has-text-centered">
              <div class="select is-large">
                <select id="example-menu" onchange="showSelectedContainer()">
                  <option value="default" selected="selected">Please select one effect.</option>
                  <option value="self-reflection">Self-Reflection</option>
                  <option value="experience-summarization">Experience Summarization</option>
                  <option value="experience-retrieval">Experience Retrieval</option>
                  <option value="grounding-with-memory">Grounding with Memory</option>
                  <option value="image-annotation">Image Annotation</option>
                </select>
              </div>
            </div>
          </div>

          <div id="default-container">
            <p>Please select a module from the dropdown to view its details.</p>
          </div>

<div class="module-container" id="self-reflection-container" style="display:none;">
  <h2 class="title is-4">Instruction: Put the apple on the plate.</h2>

  <table class="table is-striped is-four-fifths" id="tab:stm_building2">
    <thead>
      <tr>
        <th style="width: 30%;">Scene</th>
        <th style="width: 70%;">VLM Response</th>
      </tr>
    </thead>
    <tbody>
      <!-- Step 1: Pick Object (Initial Plan) -->
      <tr>
        <td><img src="./static/images/stm_apple_can/apple_can_1.png"
            alt="Apple next to salt container, plate on left"></td>
        <td class="code">
Response from VLM Task Planner: {
    "name": "pick_object",
    "arguments": {
        "scene_description": "The apple is on the right side of the table, next to a salt container. The plate is on the left side of the table, with ample space around it.",
        "reasoning": "The apple needs to be picked up first to be placed on the plate. The apple is isolated enough to be picked without interference from the salt container.",
        "target_object": "apple",
        "grasp_part": "",
        "specific_grasp_required": false,
        "action_description": "Pick up the apple carefully to avoid dropping it."
    }
}
        </td>
      </tr>

      <!-- Step 2: Success Detector - Pick Failed -->
      <tr>
        <td><img src="./static/images/stm_apple_can/apple_can_2.png" alt="Robot failed to pick up apple"></td>
        <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action to detect is 'pick up the apple.' In the image, the apple is on the table, and the robot's gripper is open and positioned in front of the table. The apple is not currently in the gripper, indicating that the action to pick up the apple has not yet been completed. Additionally, there is a cylindrical container (possibly salt) placed very close to the apple, which could obstruct the gripper's ability to securely grasp the apple, especially given the limited precision of a two-finger gripper. The plate is on the table and is not blocking the apple. To proceed, the robot should first move the cylindrical container away from the apple to clear the area for a successful grasp.",
        "action_status": "failed",
        "failure_cause": "The apple is not picked up, and a nearby container may obstruct the gripper.",
        "next_step_suggestions": "Move the cylindrical container away from the apple to clear the grasping area, then attempt to pick up the apple again.",
        "is_task_completed": false
    }
}
        </td>
      </tr>

      <!-- Step 3: Task Planner - Push Object -->
      <tr>
        <td></td>
        <td class="code">
Response from VLM Task Planner: {
    "name": "push_object",
    "arguments": {
        "scene_description": "The apple is on the right side of the table, next to a cylindrical salt container. The plate is on the left side of the table, with ample space around it. The robot's gripper is positioned in front of the table.",
        "reasoning": "The salt container is obstructing the apple, making it difficult for the robot to grasp the apple securely. Pushing the salt container to the right will clear the area around the apple, allowing for a successful grasp.",
        "object_to_push": "salt container",
        "push_direction": "right",
        "action_description": "Push the salt container to the right to clear the area around the apple."
    }
}
        </td>
      </tr>

      <!-- Step 4: Success Detector - Push Successful -->
      <tr>
        <td><img src="./static/images/stm_apple_can/apple_can_3.png"
            alt="Salt container pushed right, space around apple"></td>
        <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action to detect is pushing the salt container to the right to clear the area around the apple. In the current image, the salt container is already positioned to the right and is not blocking the apple. There is a clear space between the apple and the salt container, and the apple is not obstructed by any other objects. The area around the apple is clear, so the action of clearing the area around the apple by moving the salt container has been successfully completed.",
        "action_status": "successful",
        "failure_cause": "No objects are blocking the apple; the salt container is already to the right.",
        "next_step_suggestions": "Proceed to pick up the apple and place it on the plate as instructed.",
        "is_task_completed": false
    }
}
        </td>
      </tr>

      <!-- Step 5: Task Planner - Pick Again -->
      <tr>
        <td></td>
        <td class="code">
Response from VLM Task Planner: {
    "name": "pick_object",
    "arguments": {
        "scene_description": "The apple is on the right side of the table, with a salt container further to the right. The plate is on the left side of the table, with ample space around it.",
        "reasoning": "The salt container has been moved, clearing the area around the apple. The apple can now be picked up without obstruction. Picking up the apple is the next step to place it on the plate.",
        "target_object": "apple",
        "grasp_part": "",
        "specific_grasp_required": false,
        "action_description": "Pick up the apple carefully to avoid dropping it."
    }
}
        </td>
      </tr>

      <!-- Step 6: Success Detector - Pick Successful -->
      <tr>
        <td><img src="./static/images/stm_apple_can/apple_can_4.png" alt="Robot holding apple"></td>
        <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The robot's two-finger gripper is currently holding the apple securely. There are no objects blocking the apple, and the apple is not obstructed by any other items on the table. The action to 'pick up the apple' has been completed successfully. The next step is to move the apple and place it on the plate, as per the user's instruction.",
        "action_status": "successful",
        "failure_cause": "N/A",
        "next_step_suggestions": "Move the gripper holding the apple over the plate and release the apple onto the plate.",
        "is_task_completed": false
    }
}
        </td>
      </tr>

      <!-- Step 7: Task Planner - Place Object -->
      <tr>
        <td></td>
        <td class="code">
Response from VLM Task Planner: {
    "name": "place_object",
    "arguments": {
        "scene_description": "The apple is held by the robot's gripper, positioned in front of the table. The plate is on the left side of the table, with ample space around it. The salt container is on the right side of the table, away from the plate.",
        "reasoning": "The apple is already securely held by the robot's gripper. The plate is on the left side of the table, and there is ample space to place the apple without obstruction. The robot should move the apple over the plate and release it gently to complete the task.",
        "target_object": "apple",
        "placement_location": "plate",
        "precise_placement_spot_required": false,
        "action_description": "Move the apple over the plate and release it gently onto the plate."
    }
}
        </td>
      </tr>

      <!-- Step 8: Success Detector - Task Complete -->
      <tr>
        <td><img src="./static/images/stm_apple_can/apple_can_5.png" alt="Apple placed on plate"></td>
        <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action was to move the apple over the plate and release it gently onto the plate. In the image, the apple is already on the plate, and there is ample space around the plate. The salt container is far from the plate and does not obstruct the apple or the plate. The robot's gripper is empty, indicating that the apple has already been released. There are no objects blocking the plate or the apple. The task appears to be completed as intended.",
        "action_status": "successful",
        "failure_cause": "No failure detected; the apple is on the plate as instructed.",
        "next_step_suggestions": "No further action needed; task is complete.",
        "is_task_completed": true
    }
}
        </td>
      </tr>
    </tbody>
  </table>
</div>

<div class="module-container" id="experience-summarization-container" style="display:none;">
  <h2 class="title is-4">Instruction: Put the apple on the plate.</h2>

  <table class="table is-striped is-four-fifths" id="tab:stm_summary">
    <thead>
      <tr>
        <th style="width: 60%;">STM Logs</th>
        <th style="width: 40%;">STM Summary</th>
      </tr>
    </thead>
    <tbody>
      <!-- Step 1: Pick Object (Initial Plan) -->
      <tr>
        <td class="code">
{
    "name": "pick_object",
    "arguments": {
        "scene_description": "The apple is on the right side of the table, next to a salt container. The plate is on the left side of the table, with ample space around it.",
        "reasoning": "The apple needs to be picked up first to be placed on the plate. The apple is isolated enough to be picked without interference from the salt container.",
        "target_object": "apple",
        "grasp_part": "",
        "specific_grasp_required": false,
        "action_description": "Pick up the apple carefully to avoid dropping it."
    }
}

{
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action to detect is 'pick up the apple.' In the image, the apple is on the table, and the robot's gripper is open and positioned in front of the table. The apple is not currently in the gripper, indicating that the action to pick up the apple has not yet been completed. Additionally, there is a cylindrical container (possibly salt) placed very close to the apple, which could obstruct the gripper's ability to securely grasp the apple, especially given the limited precision of a two-finger gripper. The plate is on the table and is not blocking the apple. To proceed, the robot should first move the cylindrical container away from the apple to clear the area for a successful grasp.",
        "action_status": "failed",
        "failure_cause": "The apple is not picked up, and a nearby container may obstruct the gripper.",
        "next_step_suggestions": "Move the cylindrical container away from the apple to clear the grasping area, then attempt to pick up the apple again.",
        "is_task_completed": false
    }
}

{
    "name": "push_object",
    "arguments": {
        "scene_description": "The apple is on the right side of the table, next to a cylindrical salt container. The plate is on the left side of the table, with ample space around it. The robot's gripper is positioned in front of the table.",
        "reasoning": "The salt container is obstructing the apple, making it difficult for the robot to grasp the apple securely. Pushing the salt container to the right will clear the area around the apple, allowing for a successful grasp.",
        "object_to_push": "salt container",
        "push_direction": "right",
        "action_description": "Push the salt container to the right to clear the area around the apple."
    }
}

{
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action to detect is pushing the salt container to the right to clear the area around the apple. In the current image, the salt container is already positioned to the right and is not blocking the apple. There is a clear space between the apple and the salt container, and the apple is not obstructed by any other objects. The area around the apple is clear, so the action of clearing the area around the apple by moving the salt container has been successfully completed.",
        "action_status": "successful",
        "failure_cause": "No objects are blocking the apple; the salt container is already to the right.",
        "next_step_suggestions": "Proceed to pick up the apple and place it on the plate as instructed.",
        "is_task_completed": false
    }
}

{
    "name": "pick_object",
    "arguments": {
        "scene_description": "The apple is on the right side of the table, with a salt container further to the right. The plate is on the left side of the table, with ample space around it.",
        "reasoning": "The salt container has been moved, clearing the area around the apple. The apple can now be picked up without obstruction. Picking up the apple is the next step to place it on the plate.",
        "target_object": "apple",
        "grasp_part": "",
        "specific_grasp_required": false,
        "action_description": "Pick up the apple carefully to avoid dropping it."
    }
}

{
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The robot's two-finger gripper is currently holding the apple securely. There are no objects blocking the apple, and the apple is not obstructed by any other items on the table. The action to 'pick up the apple' has been completed successfully. The next step is to move the apple and place it on the plate, as per the user's instruction.",
        "action_status": "successful",
        "failure_cause": "N/A",
        "next_step_suggestions": "Move the gripper holding the apple over the plate and release the apple onto the plate.",
        "is_task_completed": false
    }
}

{
    "name": "place_object",
    "arguments": {
        "scene_description": "The apple is held by the robot's gripper, positioned in front of the table. The plate is on the left side of the table, with ample space around it. The salt container is on the right side of the table, away from the plate.",
        "reasoning": "The apple is already securely held by the robot's gripper. The plate is on the left side of the table, and there is ample space to place the apple without obstruction. The robot should move the apple over the plate and release it gently to complete the task.",
        "target_object": "apple",
        "placement_location": "plate",
        "precise_placement_spot_required": false,
        "action_description": "Move the apple over the plate and release it gently onto the plate."
    }
}

{
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action was to move the apple over the plate and release it gently onto the plate. In the image, the apple is already on the plate, and there is ample space around the plate. The salt container is far from the plate and does not obstruct the apple or the plate. The robot's gripper is empty, indicating that the apple has already been released. There are no objects blocking the plate or the apple. The task appears to be completed as intended.",
        "action_status": "successful",
        "failure_cause": "No failure detected; the apple is on the plate as instructed.",
        "next_step_suggestions": "No further action needed; task is complete.",
        "is_task_completed": true
    }
}
        </td>
        <td class="code">
Response from VLM Experience Summarizer: {
    "name": "summarize_robot_experience",
    "arguments": {
        "summary": "The robot was tasked with placing an apple on a plate. Initially, the apple was on a cluttered table next to a salt container, which obstructed the robot's ability to grasp the apple. The robot first attempted to pick up the apple but failed due to the obstruction. It then successfully pushed the salt container to the right, clearing the area around the apple. With the obstruction removed, the robot was able to pick up the apple securely. Finally, the robot moved the apple over the plate and released it gently, completing the task successfully. This experience highlights the importance of clearing obstructions before attempting to grasp objects, especially when using a two-finger gripper."
    }
}

        </td>
      </tr>
    </tbody>
  </table>
</div>


<div class="module-container" id="experience-retrieval-container" style="display:none;">
  <h2 class="title is-4">Instruction: Put the tennis ball on the plate.</h2>

  <table class="table is-striped is-four-fifths" id="tab:ltm_generalization2">
    <thead>
      <tr>
        <th style="width: 30%;">Scene</th>
        <th style="width: 70%;">Retrieved LTM</th>
      </tr>
    </thead>
    <tbody>
      <!-- Row: Tennis Ball Scenario -->
      <tr>
        <td>
          <img src="./static/images/ltm_tennis_ball/tennisball_1.png" alt="Tennis ball on table" style="width:100%;">
        </td>
        <td class="code">
<strong>long-term memory 1</strong>
Instruction: Put the apple on the plate.
Scene: The apple is on the right side of the table, next to a salt container. The plate is on the left side of the table, with ample space around it.
Experience: The robot was tasked with placing an apple on a plate. Initially, the apple was on a cluttered table next to a salt container, which obstructed the robot's ability to grasp the apple. The robot first attempted to pick up the apple but failed due to the obstruction. It then successfully pushed the salt container to the right, clearing the area around the apple. With the obstruction removed, the robot was able to pick up the apple securely. Finally, the robot moved the apple over the plate and released it gently, completing the task successfully. This experience highlights the importance of clearing obstructions before attempting to grasp objects, especially when using a two-finger gripper.

<strong>long-term memory 2</strong>
Instruction: Place the spoon next to the plate.
Scene: The spoon was already in the robot's gripper, and the plate was centered on the table.
Experience: The robot placed the spoon in the middle of the plate.

<strong>long-term memory 3</strong>
Instruction: Move the egg close to the banana.
Scene: The egg is positioned in the center of the table, while the bananas are on the right side. There is ample space around the egg for grasping.
Experience: The robot was tasked with moving an egg close to a banana. Initially, the robot attempted to pick up the egg using its gripper, targeting the center for a stable hold. However, the egg broke during this attempt due to its fragility, highlighting the inappropriateness of using a grasping method for fragile objects. The failure was not due to spatial constraints but rather the method used. The robot then received feedback to use a gentler approach, such as pushing, for fragile items. In the subsequent attempt, the robot successfully pushed the egg gently towards the banana, completing the task without any issues. This experience emphasizes the importance of selecting appropriate handling methods based on the object's fragility.

<strong>long-term memory 4</strong>
Instruction: Place the remote on the table.
Scene: The robot was already holding the remote, and the table surface was empty.
Experience: The robot positioned the remote over the table area within arm’s reach and released it flat onto the surface.

<strong>long-term memory 5</strong>
Instruction: Place the napkin on the tray.
Scene: The napkin was in the robot’s gripper and the tray was centered on the table.
Experience: The robot successfully released the napkin flat onto the tray.
        </td>
      </tr>
    </tbody>
  </table>
</div>


<div class="module-container" id="grounding-with-memory-container" style="display:none;">
  <h2 class="title is-4">Instruction: Put the tennis ball on the plate.</h2>

  <table class="table is-striped is-four-fifths" id="tab:grounding-with-memory">
    <thead>
      <tr>
        <th style="width: 30%;">Scene</th>
        <th style="width: 70%;">VLM Response</th>
      </tr>
    </thead>
    <tbody>
      <!-- Step 1: Push Object (Initial Plan) -->
      <tr>
        <td><img src="./static/images/ltm_tennis_ball/tennisball_1.png" alt="Tennis ball next to fan"></td>
        <td class="code">
Response from VLM Task Planner: {
    "name": "push_object",
    "arguments": {
        "scene_description": "The tennis ball is next to a fan on the right side of the table. The plate is on the left side of the table.",
        "reasoning": "The fan is obstructing the path to the tennis ball. Pushing the fan to the right will clear the path, allowing the robot to pick up the tennis ball without obstruction.",
        "object_to_push": "fan",
        "push_direction": "right",
        "action_description": "Push the fan to the right to clear the path to the tennis ball."
    }
}
        </td>
      </tr>

      <!-- Step 2: Success Detector - Push Successful (but unnecessary) -->
      <tr>
        <td><img src="./static/images/ltm_tennis_ball/tennisball_2.png" alt="Fan already to the right, path clear"></td>
        <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The action to detect was to push the fan to the right to clear the path to the tennis ball. In the current image, the fan is still positioned to the right of the tennis ball, and the tennis ball is not blocked by the fan. The path from the robot to the tennis ball appears clear, so the action of clearing the path by pushing the fan seems unnecessary or already achieved. The tennis ball is accessible, and the next step should be to grasp the tennis ball and move it toward the plate on the left side of the table.",
        "action_status": "successful",
        "failure_cause": "No failure; the path to the tennis ball is already clear.",
        "next_step_suggestions": "Proceed to grasp the tennis ball and move it toward the plate.",
        "is_task_completed": false
    }
}
        </td>
      </tr>

      <!-- Step 3: Task Planner - Pick Object -->
      <tr>
        <td></td>
        <td class="code">
Response from VLM Task Planner: {
    "name": "pick_object",
    "arguments": {
        "scene_description": "The tennis ball is on the table, with a plate on the left and a fan on the right. The path to the tennis ball is clear.",
        "reasoning": "The path to the tennis ball is clear, and there are no obstructions. The robot can directly pick up the tennis ball without needing to move any other objects.",
        "target_object": "tennis ball",
        "grasp_part": "",
        "specific_grasp_required": false,
        "action_description": "Pick up the tennis ball from the table."
    }
}
        </td>
      </tr>

      <!-- Step 4: Success Detector - Pick Successful -->
      <tr>
        <td><img src="./static/images/ltm_tennis_ball/tennisball_3.png" alt="Robot holding tennis ball"></td>
        <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The robot's two-finger gripper is currently holding the tennis ball securely. The tennis ball has been successfully picked up from the table, as indicated by its position within the gripper.",
        "action_status": "successful",
        "failure_cause": "N/A - The action was successful.",
        "next_step_suggestions": "Move the gripper holding the tennis ball over the plate and release the ball onto the plate.",
        "is_task_completed": false
    }
}
        </td>
      </tr>

      <!-- Step 5: Task Planner - Place Object -->
      <tr>
        <td></td>
        <td class="code">
Response from VLM Task Planner: {
    "name": "place_object",
    "arguments": {
        "scene_description": "The robot is holding a tennis ball in its gripper. The plate is on the left side of the table, and there is a fan on the right side. The table is otherwise clear.",
        "reasoning": "The robot has successfully picked up the tennis ball and needs to place it on the plate. The plate is clearly visible and accessible, and there are no obstacles in the way. The robot should move the gripper over the plate and release the ball gently to ensure it lands on the plate.",
        "target_object": "tennis ball",
        "placement_location": "plate",
        "precise_placement_spot_required": false,
        "action_description": "Move the gripper holding the tennis ball over the plate and release the ball onto the plate."
    }
}
        </td>
      </tr>

      <!-- Step 6: Success Detector - Task Complete -->
      <tr>
        <td><img src="./static/images/ltm_tennis_ball/tennisball_4.png" alt="Tennis ball placed on plate"></td>
        <td class="code">
Response from VLM Success Detector: {
    "name": "evaluate_action_status_and_issues",
    "arguments": {
        "reasoning": "The image shows the tennis ball already placed on the plate, and the robot's gripper is empty and positioned above the table. The plate is on the left side of the table, and the fan is on the right side. There are no visible obstacles or issues with the placement. The tennis ball is securely on the plate, which matches the user's instruction to put the tennis ball on the plate.",
        "action_status": "successful",
        "failure_cause": "No failure; the tennis ball is on the plate as instructed.",
        "next_step_suggestions": "No further action needed; task is complete.",
        "is_task_completed": true
    }
}
        </td>
      </tr>
    </tbody>
  </table>
</div>


<div class="module-container" id="image-annotation-container" style="display:none;">
  <table class="table is-striped is-four-fifths" id="tab:image-annotation-comparison">
    <thead>
      <tr>
        <th style="width: 50%;">Picking objects without annotation</th>
        <th style="width: 50%;">Picking objects with annotation</th>
      </tr>
    </thead>
    <tbody>

      <tr>
        <td>
          <img src="./static/images/image_annotation/drumstick_no_annotation.png" >
        </td>
        <td>
          <img src="./static/images/image_annotation/drumstick_annotation.png">
        </td>
      </tr>

      <tr>
        <td>
          <img src="./static/images/image_annotation/skewer_no_annotation.png">
        </td>
        <td>
          <img src="./static/images/image_annotation/skewer_annotation.png">
        </td>
      </tr>

      <tr>
        <td>
            <img src="./static/images/image_annotation/brush_no_annotation.png">
        </td>
        <td>
            <img src="./static/images/image_annotation/brush_annotation.png">
        </td>
      </tr>

    </tbody>
  </table>
</div>

        </div>
      </div>
    </section>
  </section>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
          </div>
    </section> -->


  <!-- Concurrent Work. -->
  <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an
            idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a
              href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a
              href="https://video-nerf.github.io/">Video-NeRF</a>, <a
              href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a
              href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a
              href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a
              href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
  <!--/ Concurrent Work. -->

  </div>

  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io"><span
                  class="dnerf">Nerfies</span></a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>